{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2332c0a-df65-43cf-be57-a5f73d8fe3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama_stack_client\n",
      "  Downloading llama_stack_client-0.2.12-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting fire\n",
      "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (4.9.0)\n",
      "Collecting click (from llama_stack_client)\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (0.28.1)\n",
      "Collecting pandas (from llama_stack_client)\n",
      "  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: prompt-toolkit in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (3.0.51)\n",
      "Collecting pyaml (from llama_stack_client)\n",
      "  Downloading pyaml-25.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (2.12.4)\n",
      "Collecting rich (from llama_stack_client)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: sniffio in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (1.3.1)\n",
      "Collecting termcolor (from llama_stack_client)\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/app-root/lib64/python3.11/site-packages (from anyio<5,>=3.5.0->llama_stack_client) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib64/python3.11/site-packages (from httpx<1,>=0.23.0->llama_stack_client) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/app-root/lib64/python3.11/site-packages (from httpx<1,>=0.23.0->llama_stack_client) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/app-root/lib64/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->llama_stack_client) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/app-root/lib64/python3.11/site-packages (from pydantic<3,>=1.9.0->llama_stack_client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/app-root/lib64/python3.11/site-packages (from pydantic<3,>=1.9.0->llama_stack_client) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/app-root/lib64/python3.11/site-packages (from pydantic<3,>=1.9.0->llama_stack_client) (0.4.2)\n",
      "Collecting numpy>=1.23.2 (from pandas->llama_stack_client)\n",
      "  Downloading numpy-2.3.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama_stack_client) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->llama_stack_client)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->llama_stack_client)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: wcwidth in /opt/app-root/lib64/python3.11/site-packages (from prompt-toolkit->llama_stack_client) (0.2.13)\n",
      "Requirement already satisfied: PyYAML in /opt/app-root/lib64/python3.11/site-packages (from pyaml->llama_stack_client) (6.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->llama_stack_client)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/app-root/lib64/python3.11/site-packages (from rich->llama_stack_client) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->llama_stack_client)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama_stack_client) (1.17.0)\n",
      "Downloading llama_stack_client-0.2.12-py3-none-any.whl (340 kB)\n",
      "Downloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
      "Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m201.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyaml-25.7.0-py3-none-any.whl (26 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading numpy-2.3.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m219.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, tzdata, termcolor, pyaml, numpy, mdurl, click, pandas, markdown-it-py, fire, rich, llama_stack_client\n",
      "Successfully installed click-8.3.0 fire-0.7.1 llama_stack_client-0.2.12 markdown-it-py-4.0.0 mdurl-0.1.2 numpy-2.3.4 pandas-2.3.3 pyaml-25.7.0 pytz-2025.2 rich-14.2.0 termcolor-3.2.0 tzdata-2025.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama_stack_client fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f1c9252-4f0a-42c0-9a7c-2a06b3a758fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://lsd-llama-inference-only-service-walkguardianai-llm.apps.cluster-pzdb5.pzdb5.sandbox5281.opentlc.com/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import RAGDocument, LlamaStackClient\n",
    "\n",
    "MODEL_NAME = \"granite-40-h-1b\"\n",
    "BASE_URL = \"http://lsd-llama-inference-only-service-walkguardianai-llm.apps.cluster-pzdb5.pzdb5.sandbox5281.opentlc.com\" \n",
    "\n",
    "\n",
    "client = LlamaStackClient(base_url=BASE_URL)\n",
    "models = client.models.list()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82d54d74-2ea8-4456-b320-4f0fc684364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(identifier='granite-40-h-1b', metadata={}, api_model_type='llm', provider_id='vllm-inference', type='model', provider_resource_id='granite-40-h-1b', model_type='llm')\n",
      "Model(identifier='granite-embedding-125m', metadata={'embedding_dimension': 768.0}, api_model_type='embedding', provider_id='sentence-transformers', type='model', provider_resource_id='ibm-granite/granite-embedding-125m-english', model_type='embedding')\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a092c53a-61b1-4d50-aef0-27388e903975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'granite-40-h-1b'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = next(m.identifier for m in models if m.model_type == \"llm\")\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ba4eb2e-a41e-454a-9b1c-dda8963104e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-inference-only-service-walkguardianai-llm.apps.cluster-pzdb5.pzdb5.sandbox5281.opentlc.com/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "danger_level: 8\n",
      "danger_type: physical_threat\n",
      "summary: The user expresses a clear intention to harm someone, using strong language and a threat to kill. The rapid escalation of the conversation and the explicit threat indicate a high level of danger.\n",
      "recommended_action: Immediately take evasive action, seek help from authorities, and avoid the area. Do not confront the person or engage in further dialogue.\n"
     ]
    }
   ],
   "source": [
    "transcript = \"\"\"\n",
    "[00:14] “Someone's following us…”\n",
    "[00:17] “No, seriously, he’s getting closer.”\n",
    "[00:21] I'm going to kill you! You better run.\n",
    "\"\"\"\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=\"granite-40-h-1b\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "            You are a safety-analysis model. Your task is to determine whether the user is currently in danger based on an audio transcript.\n",
    "\n",
    "            The input will be a transcript containing:\n",
    "            - Time-stamped segments  \n",
    "            - Spoken words from multiple speakers  \n",
    "            - Nonverbal cues when available (e.g., footsteps, glass breaking, screams)\n",
    "            \n",
    "            Your analysis must be highly accurate, objective, and concise.\n",
    "            \n",
    "            ### OUTPUT FORMAT (strict):\n",
    "            danger_level: <1–10>\n",
    "            danger_type: <one of the categories below>\n",
    "            summary: <1–3 sentences explaining the reasoning>\n",
    "            recommended_action: <clear, practical, and safe advice>\n",
    "            \n",
    "            ### Danger Level Scale:\n",
    "            1–3: low risk (normal conversation, no signs of danger)\n",
    "            4–6: moderate risk (concerning cues; monitor the situation)\n",
    "            7–8: high risk (possible immediate threat)\n",
    "            9–10: extreme risk (urgent danger; may require emergency action)\n",
    "            \n",
    "            ### Allowed Danger Types:\n",
    "            - medical_distress\n",
    "            - physical_threat\n",
    "            - stalking_or_following\n",
    "            - domestic_dispute\n",
    "            - verbal_aggression\n",
    "            - possible_theft\n",
    "            - intoxication_or_impairment\n",
    "            - lost_or_disoriented\n",
    "            - mental_health_crisis\n",
    "            - environmental_hazard (e.g., fire, car traffic, loud crashes)\n",
    "            - unknown (if not enough information)\n",
    "            \n",
    "            Your output must always be structured exactly as specified, with no additional commentary.\n",
    "            \n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": transcript\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.completion_message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
