{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query a Deployed Model on Red Hat OpenShift AI\n",
    "\n",
    "This notebook demonstrates how to connect to and query a large language model that has already been deployed using the Single-Model Serving feature in a Red Hat OpenShift AI data science project. \n",
    "\n",
    "We will use the `langchain-openai` library to create a client that can communicate with any OpenAI-compatible API endpoint, which is what the vLLM runtime provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries\n",
    "\n",
    "First, we need to install the necessary Python libraries. `langchain-openai` provides the tools to interact with the model endpoint, and `httpx` is the underlying HTTP client used to make the requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-1.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: httpx in /opt/app-root/lib64/python3.11/site-packages (0.28.1)\n",
      "Collecting langchain-core<2.0.0,>=1.0.2 (from langchain-openai)\n",
      "  Downloading langchain_core-1.0.5-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting openai<3.0.0,>=1.109.1 (from langchain-openai)\n",
      "  Downloading openai-2.8.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting tiktoken<1.0.0,>=0.7.0 (from langchain-openai)\n",
      "  Downloading tiktoken-0.12.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: anyio in /opt/app-root/lib64/python3.11/site-packages (from httpx) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib64/python3.11/site-packages (from httpx) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/app-root/lib64/python3.11/site-packages (from httpx) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/app-root/lib64/python3.11/site-packages (from httpx) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/app-root/lib64/python3.11/site-packages (from httpcore==1.*->httpx) (0.16.0)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.0.2->langchain-openai)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.0.2->langchain-openai)\n",
      "  Downloading langsmith-0.4.42-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/app-root/lib64/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (25.0)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain-core<2.0.0,>=1.0.2->langchain-openai)\n",
      "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/app-root/lib64/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (6.0.2)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<2.0.0,>=1.0.2->langchain-openai)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/app-root/lib64/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (4.14.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai<3.0.0,>=1.109.1->langchain-openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai<3.0.0,>=1.109.1->langchain-openai)\n",
      "  Downloading jiter-0.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /opt/app-root/lib64/python3.11/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
      "Collecting tqdm>4 (from openai<3.0.0,>=1.109.1->langchain-openai)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1.0.0,>=0.7.0->langchain-openai)\n",
      "  Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/app-root/lib64/python3.11/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/app-root/lib64/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.2->langchain-openai) (3.0.0)\n",
      "Collecting orjson>=3.9.14 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai)\n",
      "  Downloading orjson-3.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai)\n",
      "  Downloading zstandard-0.25.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.2->langchain-openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.2->langchain-openai)\n",
      "  Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-extensions<5.0.0,>=4.7.0 (from langchain-core<2.0.0,>=1.0.2->langchain-openai)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.2->langchain-openai)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.4.0)\n",
      "Downloading langchain_openai-1.0.2-py3-none-any.whl (81 kB)\n",
      "Downloading langchain_core-1.0.5-py3-none-any.whl (471 kB)\n",
      "Downloading openai-2.8.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.12.0-cp311-cp311-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m575.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (364 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langsmith-0.4.42-py3-none-any.whl (401 kB)\n",
      "Downloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m211.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.4/800.4 kB\u001b[0m \u001b[31m324.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading orjson-3.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading zstandard-0.25.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m247.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: zstandard, typing-extensions, tqdm, tenacity, regex, orjson, jsonpatch, jiter, distro, annotated-types, typing-inspection, tiktoken, requests-toolbelt, pydantic-core, pydantic, openai, langsmith, langchain-core, langchain-openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.14.0\n",
      "    Uninstalling typing_extensions-4.14.0:\n",
      "      Successfully uninstalled typing_extensions-4.14.0\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 jiter-0.12.0 jsonpatch-1.33 langchain-core-1.0.5 langchain-openai-1.0.2 langsmith-0.4.42 openai-2.8.0 orjson-3.11.4 pydantic-2.12.4 pydantic-core-2.41.5 regex-2025.11.3 requests-toolbelt-1.0.0 tenacity-9.1.2 tiktoken-0.12.0 tqdm-4.67.1 typing-extensions-4.15.0 typing-inspection-0.4.2 zstandard-0.25.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-openai httpx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Model Connection\n",
    "\n",
    "Next, we need to configure the connection to our deployed Granite model. You must replace the placeholder values below with the specific details from your model's deployment page in OpenShift AI.\n",
    "\n",
    "**Action Required:**\n",
    "1.  **`BASE_URL`**: Replace the placeholder with the **Inference endpoint** URL from your model's details page.\n",
    "2.  **`API_KEY`**: Replace the placeholder with the **Authentication Token** from the 'Authentication' section of the model's details page.\n",
    "3.  **`MODEL_NAME`**: This should be the name you gave your deployment (e.g., `granite`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration successful. Client is ready.\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# --- ❗ACTION REQUIRED: REPLACE THESE VALUES ❗---\n",
    "MODEL_NAME = \"granite-40-h-1b\" # Or the specific name of your model deployment\n",
    "BASE_URL = \"insert-here\" # Replace with your model's Inference endpoint\n",
    "API_KEY = \"insert-here\"# -----------------------------------------------------\n",
    "\n",
    "# Optional: If your cluster uses self-signed certificates, you may need to disable SSL verification.\n",
    "# Note: This is not recommended for production environments.\n",
    "# http_client = httpx.Client(verify=False)\n",
    "\n",
    "try:\n",
    "    # Initialize the ChatOpenAI client\n",
    "    llm = ChatOpenAI(\n",
    "        model=MODEL_NAME,\n",
    "        api_key=API_KEY,\n",
    "        base_url=BASE_URL,\n",
    "        # Uncomment the line below if you need to disable SSL verification\n",
    "        # http_client=http_client,\n",
    "    )\n",
    "\n",
    "    print(\"Configuration successful. Client is ready.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during client initialization: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Send a Request to the Model\n",
    "\n",
    "Now that the client is configured, we can send a request to the model. We construct a list of messages, including a `SystemMessage` to set the model's behavior and a `HumanMessage` with our question. Then, we use the `llm.invoke()` method to get a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request to the Granite model...\n",
      "\n",
      "Response from Granite Model:\n",
      "OpenShift AI (originally known as KubeAI) is a platform designed by Red Hat to simplify the deployment of AI workloads on Kubernetes. It allows developers and data scientists to quickly set up and manage a complete AI stack on a Kubernetes cluster, covering everything from model training to deployment.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Prepare the messages for the model\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful assistant who provides concise answers.\"),\n",
    "        HumanMessage(content=\"What is OpenShift AI?\"),\n",
    "    ]\n",
    "\n",
    "    # Invoke the model and get the response\n",
    "    print(\"Sending request to the Granite model...\")\n",
    "    ai_msg = llm.invoke(messages)\n",
    "\n",
    "    # Print the content of the response\n",
    "    print(\"\\nResponse from Granite Model:\")\n",
    "    print(ai_msg.content)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment!\n",
    "\n",
    "Now it's your turn. Go back to the previous code cell, change the content of the `HumanMessage` to your own question, and run the cell again to see how the model responds. \n",
    "\n",
    "**Example questions:**\n",
    "* `\"Write a python function that calculates the factorial of a number.\"`\n",
    "* `\"What are the key benefits of using a GPU for deep learning?\"`\n",
    "* `\"Explain the difference between object storage and file storage.\"`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
